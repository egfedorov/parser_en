import requests
from bs4 import BeautifulSoup
from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
from urllib.parse import urljoin
import time

def parse_date(date_str: str) -> datetime:
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –≤–∏–¥–∞ '20 –æ–∫—Ç—è–±—Ä—è 2025 –≥.' (—Ä—É—Å—Å–∫–∞—è –ª–æ–∫–∞–ª—å)"""
    months = {
        "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4,
        "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8,
        "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12,
    }
    parts = date_str.replace("–≥.", "").split()
    try:
        day = int(parts[0])
        month = months[parts[1].lower()]
        year = int(parts[2])
        return datetime(year, month, day, tzinfo=timezone.utc)
    except Exception:
        return datetime.now(timezone.utc)

def get_article_date(article_url: str) -> datetime:
    """–î–æ—Å—Ç–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏"""
    try:
        r = requests.get(article_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        r.raise_for_status()
        s = BeautifulSoup(r.text, "html.parser")
        date_tag = s.select_one("p.date")
        if date_tag:
            return parse_date(date_tag.get_text(strip=True))
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to get date from {article_url}: {e}")
    return datetime.now(timezone.utc)

def generate():
    base_url = "https://macleans.ca"
    url = f"{base_url}/tag/big-stories/"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    articles = soup.select("div.__articles__Z0v1U article")

    fg = FeedGenerator()
    fg.id(url)
    fg.title("Maclean‚Äôs ‚Äî Big Stories")
    fg.link(href=url, rel="alternate")
    fg.description("Latest longform and big stories from Maclean‚Äôs")
    fg.language("en")

    print(f"üì∞ Found {len(articles)} articles. Fetching details...")

    for art in articles[:15]:
        title_tag = art.select_one("h3 a")
        excerpt_tag = art.select_one("div.excerpt")
        img_tag = art.select_one("img")
        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = urljoin(base_url, title_tag.get("href"))
        description = excerpt_tag.get_text(strip=True) if excerpt_tag else ""
        image_url = img_tag.get("src") if img_tag else ""

        pub_date = get_article_date(link)
        time.sleep(1)

        fe = fg.add_entry()
        fe.id(link)
        fe.title(title)
        fe.link(href=link)
        fe.description(description)
        if image_url:
            fe.enclosure(url=image_url, type="image/jpeg")
        fe.pubDate(pub_date)

        print(f"‚úì Parsed: {title} ‚Äî {pub_date.date()}")

    fg.rss_file("../macleans.xml", encoding="utf-8")
    print("‚úÖ macleans1.xml generated successfully")

if __name__ == "__main__":
    generate()
